# Hacker News Scraper Documentation

## Overview

This Node.js application scrapes articles from Hacker News' "newest" page to validate that articles are properly sorted in chronological order (newest to oldest). The scraper uses Playwright to automate browser interactions and collect up to 100 articles across multiple pages.

## Purpose

The primary objective is to verify that Hacker News maintains proper chronological sorting on their newest articles page. The application:
- Scrapes article titles and timestamps from multiple pages
- Validates the chronological ordering
- Provides detailed statistics and error reporting
- Handles various edge cases and error conditions

## Prerequisites

### System Requirements
- Node.js (version 14 or higher)
- npm or yarn package manager
- Sufficient system memory (recommended: 2GB+ available)

### Dependencies
```bash
npm install playwright
```

After installation, install browser binaries:
```bash
npx playwright install chromium
```

## Installation

1. **Clone or download the script file**
2. **Install dependencies:**
   ```bash
   npm install playwright
   npx playwright install chromium
   ```
3. **Run the script:**
   ```bash
   node hacker-news-scraper.js
   ```

## How It Works

### Core Functionality

1. **Browser Initialization**
   - Launches Chromium browser in headless mode
   - Sets up realistic user agent to avoid detection
   - Configures appropriate timeouts for network operations

2. **Page Navigation**
   - Starts at `https://news.ycombinator.com/newest`
   - Navigates through multiple pages using "more" links
   - Handles pagination automatically until 100 articles are collected

3. **Data Extraction**
   - Identifies article rows using CSS selector `tr.athing`
   - Extracts article titles from `.titleline a` elements
   - Retrieves timestamps from `.age` elements with fallback mechanisms
   - Tracks the page number for each article

4. **Validation**
   - Compares consecutive timestamps to detect sorting errors
   - Identifies articles that appear out of chronological order
   - Provides detailed error reporting with specific examples

### Error Handling

The application includes robust error handling for:
- **Network timeouts**: Extended timeouts for slow connections
- **Missing elements**: Fallback selectors for timestamp extraction
- **Navigation failures**: Retry mechanisms and alternative waiting strategies
- **Page load issues**: Multiple validation approaches for content availability
- **Parsing errors**: Graceful handling of malformed timestamps

## Configuration Options

### Timeout Settings
- **Default timeout**: 45 seconds for general operations
- **Navigation timeout**: 60 seconds for page transitions
- **Selector timeout**: 20 seconds for element appearance

### Collection Parameters
- **Target articles**: 100 articles across multiple pages
- **Max consecutive errors**: 3 failed attempts before stopping
- **Rate limiting**: 1-second delay between page navigations

## Output Format

### Success Output
```
✅ SUCCESS: Articles are sorted newest to oldest!

ADDITIONAL STATISTICS
Articles per page:
  Page 1: 30 articles
  Page 2: 30 articles
  Page 3: 30 articles
  Page 4: 10 articles

Newest article: 2024-01-15T10:30:00.000Z
Oldest article: 2024-01-15T08:15:00.000Z
Time range: 135.00 minutes
```

### Error Output
```
❌ FAILURE: Found 2 sorting errors

Error #1 at position 45:
  CURRENT: Article Title A (2024-01-15T09:00:00.000Z) [Page 2]
  NEXT:    Article Title B (2024-01-15T09:15:00.000Z) [Page 2]
```

### Statistics Provided
- **Validation results**: Pass/fail status with error details
- **Page distribution**: Number of articles collected per page
- **Time range analysis**: Span between newest and oldest articles
- **Sample articles**: First 5 (newest) and last 5 (oldest) articles
- **Timestamp parsing**: Success rate for date parsing

## Technical Implementation

### Key Components

1. **Browser Management**
   browser = await chromium.launch({ 
     headless: true,
     timeout: 60000
   });
   ```

2. **Element Selection**
   // Primary article selector
   const articleRows = await page.$$('tr.athing');
   
   // Timestamp extraction with fallbacks
   const timestamp = await subtextRow.evaluate(node => {
     return node.querySelector('.age')?.title || 
            node.querySelector('.age a')?.title ||
            node.querySelector('[title]')?.title ||
            null;
   });
   ```

3. **Pagination Handling**
   const moreLink = await page.$('a.morelink');
   if (moreLink) {
     await Promise.all([
       moreLink.click(),
       page.waitForNavigation({ waitUntil: 'domcontentloaded' })
     ]);
   }
   ```

### Data Structure

Each collected article contains:
{
  title: "Article Title",
  timestamp: "2024-01-15T10:30:00.000Z",
  position: 1,
  page: 1
}
```

## Limitations and Considerations

### Known Limitations
- **Rate limiting**: Hacker News may implement rate limiting for automated requests
- **Dynamic content**: Some articles may load dynamically, requiring extended wait times
- **Timestamp formats**: Various timestamp formats may not parse correctly
- **Network dependency**: Requires stable internet connection for multi-page scraping

### Performance Considerations
- **Memory usage**: Stores all articles in memory during execution
- **Execution time**: Typically 2-5 minutes depending on network speed
- **Resource usage**: Runs headless browser instance throughout execution


## Best Practices

### Usage Guidelines
- **Respectful scraping**: Includes delays between requests
- **Error recovery**: Graceful handling of failures
- **Resource cleanup**: Proper browser closure in all scenarios
- **Detailed logging**: Comprehensive output for debugging

### Code Quality
- **Error handling**: Multiple layers of exception handling
- **Type safety**: Validation of data types and null checks
- **Performance**: Efficient DOM querying and memory management
- **Maintainability**: Clear structure and comprehensive comments

## Security and Ethics

### Responsible Use
- **Rate limiting**: Built-in delays to avoid overwhelming servers
- **User agent**: Realistic browser identification
- **Public data**: Only accesses publicly available information
- **No authentication**: Does not attempt to bypass login systems

### Compliance
- **Terms of service**: Respects Hacker News' public API limitations
- **Data usage**: Collected data used only for validation purposes
- **No storage**: Data is not persisted beyond execution scope

## Output Interpretation

### Success Criteria
The scraper validates that articles are sorted newest to oldest by:
- Comparing consecutive timestamps
- Identifying any chronological inconsistencies
- Providing statistical analysis of the time distribution

### Failure Analysis
When sorting errors are detected, the output includes:
- **Error count**: Total number of sorting violations
- **Specific examples**: Detailed information about misplaced articles
- **Position tracking**: Exact location of each sorting error
- **Page context**: Which page each problematic article appeared on

## Support and Maintenance

### Regular Updates
- **Selector validation**: Periodic verification of CSS selectors
- **Timeout adjustment**: Modification based on network conditions
- **Error handling**: Enhancement of edge case coverage

### Monitoring
- **Success rate**: Track validation success across runs
- **Performance metrics**: Monitor execution time and resource usage
- **Error patterns**: Identify recurring issues for improvement
